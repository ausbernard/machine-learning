# Week 3 Notes	╭(ʘ̆~◞౪◟~ʘ̆)╮

## 3.2 - How Machines Learn

- [ ] Explore the process used to create extremely specialized AI applications.

### What is Machine Learning?
    A set of tools and practices that use data to train an AI algorithm to produce a more accurate output. Rather than programming tasks directly, it is a process where loads of input data is used to produce a solution for a task that can pick new unseen outputs.

The end product of machine learning is called **a model**. When the learning has finished one **deploys** the model into an AI application.


### The Learning Process
![ml-learning-process](images/ml-learning-process.png)
1. **Input**
- The very first step is to gather the relevant data and cleanse it using data science. 
- Can then split the data into training and test sets. Do not use all data in training set because this runs the issue of overfitting.
2. **Train**
- The model will repeatedly analyze the training data and attempt to produce the desired output, adjusting as it goes to become more accurate.
3. **Test**
- During testing, try the model using a set of test data, which is data that you have not passed to the model before, either while training it or in previous tests. If a model is very accurate for the training data but fails to produce accurate results for the test data, then the model is **overfitted**. 
- When you have not trained the model sufficiently, it will be inaccurate on both the training and test data — then the model is **underfitted**.
- Machine learning models will usually include a **confidence score** with each prediction. You can use these to gauge how well the model was trained.
    - A good confidence score during the training phase typically means that the model is consistently making confident and accurate predictions on the training data. Generally, you'd want high confidence scores, ideally close to 100%, indicating the model is very confident in its predictions.

    - On the other hand, a poor confidence score during training could mean several things:
        - Low Confidence: The model is consistently unsure about its predictions, indicating that it's struggling to learn from the data.
        - Inconsistency: The confidence scores fluctuate a lot, suggesting instability in the learning process.
        - Overfitting: The model has learned to memorize the training data rather than generalize from it, leading to overly confident but inaccurate predictions on unseen data.
        - Underfitting: The model hasn't learned enough from the training data, resulting in low confidence scores because it's uncertain about its predictions.
4. **Repeat until Sweet Spot**
    - If you are not happy with the results, you can put the model back through training. When you do this, you will often include new examples in the training set to correct errors found during testing.
    - In essence, during training, you're aiming for high and consistent confidence scores that reflect the model's ability to learn patterns effectively from the data.
5. Deployment

### How to train your machine
![ml-training-model-pattern-graph](images/ml-traning-model-pattern-graph.png)

Differ depending on the problem but almost all follow the same pattern:
1. **Predict**
    - The model will identify the patterns in the data and then use these patterns to make a prediction about the outcome for each data point. In this way, it creates rules that it uses to make predictions. To use an earlier example, your rock, paper, scissors model, it would look at each training image and make a prediction about whether it was rock, paper, or scissors.
    - It may not use all the data at one, but instead breaking it into **batches**, and you can control the **batch size** in most machine learning systems. 
    - The model is updated after each batch. The training process would be much slower if the model had to check every error and adjust after every single data point
    - When the model has gone through all the batches, and looked at all of the training data points, this is called an **epoch**. 
    - In a typical training process, there are multiple epochs, as the process is repeated until the error is as small as possible. It will run it multiple times through to find the smallest error possible (I think would be like the line of best fit for instance)
2. **Error/Loss**
    - The process after the model has made its predictions, the algorithm checks how accurate the model was.
    - The model needs some way of scoring itself; typically, this is generated by using an error or loss function that returns a single value showing how different the model's predictions were to the actual outcome.
        - Let's consider a simple example of a binary classification problem where the task is to predict whether an email is spam or not spam based on its content.
        - In this scenario, the model could output a probability score between 0 and 1, where 0 indicates the model is very confident it's not spam, and 1 indicates the model is very confident it is spam. Let's say the model's prediction for a particular email being spam is 0.8.
        - Now, to score the model's performance, we typically use a loss function, such as binary cross-entropy, which measures the difference between the predicted probabilities and the actual outcomes (labels). If the actual label for this email is indeed spam (let's say it's 1), then the error for this prediction would be calculated using the binary cross-entropy loss. 
        
        - ![probability-score](images/probability-score.png)
        - So, the model's loss for this prediction would be approximately 0.2231. 

3. **Adjust**
- Before the next iteration, the algorithm will adjust the model's rules. The size of that change will be determined by the amount of error and the learning rate you set before training starts. The learning rate determines the step size the algorithm uses to change the model's rules; the higher the error in the iteration, the more steps the algorithm will use to adjust the model.
- During training, the goal is to minimize this loss function across all predictions in the training dataset. A lower loss indicates that the model's predictions are closer to the actual outcomes, which generally corresponds to better performance. Conversely, a higher loss would indicate poorer performance.
- A large learning rate will make the training process quicker, as large steps quickly lead to plateaus in the amount of error. However, finding the perfect sweet spot requires fine adjustments, so although it may take longer, a lower learning rate tends to lead to a more accurate model.

### Knowing when to stop
There are two factors that will determine when you stop the training process:
1. The model's accuracy
1. The number of epochs

Eventually, your model will reach a point where a round of adjustment only leads to a negligible change in the error. You might also set a maximum number of iterations (or epochs) after which you will stop training.

### Key Terms
![ml-key-terms](images/ml-key-terms.png)

## 3.3 - Introduction to Supervised Learning